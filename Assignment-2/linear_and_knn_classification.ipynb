{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 551: Applied Machine Learning\n",
    "## Assignment 2: Linear  Classification  and  Nearest  Neighbor  Classification\n",
    "### Author: Antonios Valkanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "#### You will use a synthetic data set for the classification task that you’ll generate yourself. Generate two classes with 20 features each. Each class is given by a multivariate Gaussian distribution,  with both classes sharing the same covariance matrix.  You are provided with the mean vectors (DS1-m0 for mean vector of negative class and DS1-m1 for mean vector of positive class) and the covariance matrix (DS1-cov).  Generate 2000 examples for each class,  and label the data to be positive if they came from the Gaussian with mean m1 and negative if they came from the Gaussian with mean m0.  Randomly pick (without replacement) 20% of each class (i.e., 400 data points per class) as test set, 20% of each class (i.e., 400 data points per class) as validation set set and train the classifiers on the remaining 60% data.  When you report performance results, it should be on the test set.  Call this dataset as DS1, and submit it with your code.  Follow the instructions from Assignment 1 for data submission format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the mean data from file.\n",
    "mean_0 = np.loadtxt(\"Datasets/DS1_m_0.txt\", delimiter = ',', usecols=range(20))\n",
    "mean_1 = np.loadtxt(\"Datasets/DS1_m_1.txt\", delimiter = ',', usecols=range(20))\n",
    "cov = np.loadtxt(\"Datasets/DS1_Cov.txt\", delimiter = ',', usecols= range(20))\n",
    "\n",
    "# Generate the multivariate gaussian distributions and sample each 2000 times.\n",
    "negative_class = np.random.multivariate_normal(mean_0,cov,2000)\n",
    "positive_class = np.random.multivariate_normal(mean_1,cov,2000)\n",
    "\n",
    "# Add labels to the data (add a 0 or a 1 on the 21-st column for labeling)\n",
    "negative_class = np.insert(negative_class, 20, 0,axis=1)\n",
    "positive_class = np.insert(positive_class, 20, 1,axis=1)\n",
    "\n",
    "# Create train, validation and test sets from both sample distributions.\n",
    "test = np.append(negative_class[:400], positive_class[:400],axis=0)\n",
    "validation = np.append(negative_class[400:800], positive_class[400:800],axis=0)\n",
    "train = np.append(negative_class[800:], positive_class[800:],axis=0)\n",
    "\n",
    "# Shuffle datasets to randomize train, validation and test set selection.\n",
    "np.random.shuffle(test)\n",
    "np.random.shuffle(validation)\n",
    "np.random.shuffle(train)\n",
    "\n",
    "# Save data\n",
    "np.savetxt('DS1_train.csv', train, delimiter = ',')\n",
    "np.savetxt('DS1_validation.csv', validation, delimiter = ',')\n",
    "np.savetxt('DS1_test.csv', test, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "#### We first consider the GDA model as seen in class:  given the class variable, the data are assumed to be Gaussians with different means for different classes but with the same covariance matrix.  This model can formally be specified as follows:\n",
    "#### Y∼Bernoulli(π),  X|Y = j ∼ N(μj,Σ)\n",
    "#### Estimate  the  parameters  of  the  GDA  model  using  the  maximum  likelihood  approach.\n",
    "1.  For DS1, report the best fit accuracy, precision, recall and F-measure achieved by the classifier.\n",
    "2.  Report the coefficients learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients learned:\n",
      "w0: 25.873030313005195\n",
      "w: [ 13.73068895  -8.39076669  -5.75281745  -2.86687715  -9.40174385\n",
      "  -3.9064478   16.32372139 -22.91392594 -27.90111292   8.83033948\n",
      " -12.50578729 -11.98214711  14.9072422   12.22579561  -5.07489982\n",
      "  12.29990028  28.31986351  -6.31844394  -0.55656195  -4.84331305]\n"
     ]
    }
   ],
   "source": [
    "# Load train, validation and test sets.\n",
    "test = np.loadtxt('DS1_test.csv',delimiter = ',')\n",
    "validation = np.loadtxt('DS1_validation.csv',delimiter = ',')\n",
    "train = np.loadtxt('DS1_train.csv',delimiter = ',')\n",
    "\n",
    "def GDA(train):\n",
    "    # Get N1(negatives) and N2(positives) from the total N examples.\n",
    "    # Also get the sample means for μ1(m1) and μ2 (m2).\n",
    "    N = len(train)\n",
    "    N1 = 0.0\n",
    "    N2 = 0.0\n",
    "    m1 = np.zeros(20) #len(train[0] - 1)\n",
    "    m2 = np.zeros(20)\n",
    "\n",
    "    for row in train:\n",
    "        if row[-1] == 0:\n",
    "            N1 += 1\n",
    "            m1 += row[:-1]\n",
    "        else:\n",
    "            N2 += 1\n",
    "            m2 += row[:-1]     \n",
    "\n",
    "    m1 /= N1\n",
    "    m2 /= N2\n",
    "    p1 = N1/N\n",
    "    p2 = 1 - p1\n",
    "\n",
    "    #To get covariance matrix we need S1 and S2.\n",
    "    row_mean = 0\n",
    "    S1 = 0\n",
    "    S2 = 0\n",
    "    for row in train:\n",
    "        if row[-1]==0:\n",
    "            row_mean = np.array(row[:-1]) - m1\n",
    "            row_mean = np.reshape(row_mean,(20,1))       \n",
    "            S1 += row_mean.dot(row_mean.T)\n",
    "        else:\n",
    "            row_mean = np.array(row[:-1]) - m2\n",
    "            row_mean = np.reshape(row_mean,(20,1))       \n",
    "            S2 += row_mean.dot(row_mean.T)\n",
    "\n",
    "    S1 /= N1\n",
    "    S2 /= N2\n",
    "\n",
    "    # Use S1, S2 to get covariance matrix\n",
    "    cov_matrix = p1*S1 + p2*S2\n",
    "\n",
    "    # Use formulae form lecture to obtain w0 and w1 decision boundary parameters.\n",
    "    # ω1 = cov^-1(μ1 - μ2)\n",
    "    # ω0 = -0.5*μ1^T*cov^-1*μ1 + 0.5*μ2^T*cov^-1*μ2 + ln(N1/N2)\n",
    "    inverse_cov = np.linalg.inv(cov_matrix)\n",
    "    w = inverse_cov.dot(m1-m2)\n",
    "    w0 = -0.5 * (m1.T).dot(inverse_cov).dot(m1) + \\\n",
    "    0.5 * (m2.T).dot(inverse_cov).dot(m2) + np.log(N1/N2)\n",
    "\n",
    "    print(\"Coefficients learned:\")\n",
    "    print('w0:', w0)\n",
    "    print('w:', w)\n",
    "    return w0, w\n",
    "\n",
    "w0, w = GDA(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for calulations of model prediction.\n",
    "\n",
    "# Activation function is a sigmoid.\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Evaluate linear model and pass through activation function.\n",
    "def probability_negative(x, w0, w):\n",
    "    linear_model = w0 + w.dot(x)\n",
    "    return sigmoid(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96375\n",
      "Precision: 0.9557739557739557\n",
      "Recall: 0.9725\n",
      "F measure: 0.9640644361833953\n"
     ]
    }
   ],
   "source": [
    "# Get model precision, accuracy, recall and F1.\n",
    "def test_GDA(test, w0, w):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    for row in test:\n",
    "        # True negative case\n",
    "        if row[-1] == 0:\n",
    "            if probability_negative(row[:-1], w0, w) >= 0.5:\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        # True positive case\n",
    "        else:\n",
    "            if probability_negative(row[:-1], w0, w) < 0.5:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "\n",
    "    precision = true_positives/(true_positives + false_positives)\n",
    "    recall = true_positives/(true_positives + false_negatives)\n",
    "    accuracy = (true_positives + true_negatives)/(true_positives + false_negatives + true_negatives + false_positives)\n",
    "    f = 2*precision*recall/(precision+recall)\n",
    "    print('Accuracy:',accuracy)\n",
    "    print('Precision:',precision)\n",
    "    print('Recall:',recall)\n",
    "    print('F measure:',f)\n",
    "    \n",
    "test_GDA(test, w0, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "#### For DS1, use k-NN to learn a classifier.  Repeat the experiment for different values of k and report the performance for each value.  We will compare this non-linear classifier to the linear approach, and find out how powerful linear classifiers can be. \n",
    "1.  Does this classifier perform better than GDA or worse?  Are there particular values of k which perform better?  Why does this happen ?  Use F1-Measure for model selection.\n",
    "2. Report the best fit accuracy, precision, recall and f-measure achieved by this classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN(x,k):\n",
    "    neighbors = []\n",
    "    prob_positive = 0.0\n",
    "    for i, row in enumerate(train):\n",
    "        distance = np.linalg.norm(x - row[:-1])\n",
    "        if len(neighbors) < k:\n",
    "            neighbors.append([distance, i])\n",
    "        else:\n",
    "            neighbors = sorted(neighbors, key = lambda x: x[0])\n",
    "            if distance < neighbors[k-1][0]:\n",
    "                neighbors[-1] = [distance,i]       \n",
    "    for neighbor in neighbors:\n",
    "        prob_positive += train[neighbor[1]][-1]\n",
    "    prob_positive /= k \n",
    "    return prob_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 1\n",
      "Accuracy: 0.50875\n",
      "Precision: 0.5084745762711864\n",
      "Recall: 0.525\n",
      "F measure: 0.5166051660516606\n",
      "\n",
      "For k = 2\n",
      "Accuracy: 0.51\n",
      "Precision: 0.5185185185185185\n",
      "Recall: 0.28\n",
      "F measure: 0.36363636363636365\n",
      "\n",
      "For k = 3\n",
      "Accuracy: 0.53375\n",
      "Precision: 0.5338345864661654\n",
      "Recall: 0.5325\n",
      "F measure: 0.5331664580725908\n",
      "\n",
      "For k = 4\n",
      "Accuracy: 0.54125\n",
      "Precision: 0.56\n",
      "Recall: 0.385\n",
      "F measure: 0.4562962962962963\n",
      "\n",
      "For k = 5\n",
      "Accuracy: 0.525\n",
      "Precision: 0.524390243902439\n",
      "Recall: 0.5375\n",
      "F measure: 0.5308641975308641\n",
      "\n",
      "For k = 6\n",
      "Accuracy: 0.53875\n",
      "Precision: 0.5508196721311476\n",
      "Recall: 0.42\n",
      "F measure: 0.4765957446808511\n",
      "\n",
      "For k = 7\n",
      "Accuracy: 0.5425\n",
      "Precision: 0.541871921182266\n",
      "Recall: 0.55\n",
      "F measure: 0.5459057071960298\n",
      "\n",
      "For k = 8\n",
      "Accuracy: 0.52625\n",
      "Precision: 0.5329153605015674\n",
      "Recall: 0.425\n",
      "F measure: 0.4728789986091794\n",
      "\n",
      "For k = 9\n",
      "Accuracy: 0.54375\n",
      "Precision: 0.543424317617866\n",
      "Recall: 0.5475\n",
      "F measure: 0.5454545454545454\n",
      "\n",
      "For k = 20\n",
      "Accuracy: 0.535\n",
      "Precision: 0.5362694300518135\n",
      "Recall: 0.5175\n",
      "F measure: 0.5267175572519084\n",
      "\n",
      "For k = 30\n",
      "Accuracy: 0.54875\n",
      "Precision: 0.5498721227621484\n",
      "Recall: 0.5375\n",
      "F measure: 0.5436156763590393\n",
      "\n",
      "For k = 40\n",
      "Accuracy: 0.5375\n",
      "Precision: 0.5380710659898477\n",
      "Recall: 0.53\n",
      "F measure: 0.5340050377833754\n",
      "\n",
      "For k = 50\n",
      "Accuracy: 0.54\n",
      "Precision: 0.5388349514563107\n",
      "Recall: 0.555\n",
      "F measure: 0.5467980295566501\n",
      "\n",
      "For k = 60\n",
      "Accuracy: 0.5425\n",
      "Precision: 0.5389908256880734\n",
      "Recall: 0.5875\n",
      "F measure: 0.562200956937799\n",
      "\n",
      "For k = 70\n",
      "Accuracy: 0.54375\n",
      "Precision: 0.5393258426966292\n",
      "Recall: 0.6\n",
      "F measure: 0.5680473372781064\n",
      "\n",
      "For k = 80\n",
      "Accuracy: 0.55375\n",
      "Precision: 0.5474613686534217\n",
      "Recall: 0.62\n",
      "F measure: 0.5814771395076201\n",
      "\n",
      "For k = 90\n",
      "Accuracy: 0.55125\n",
      "Precision: 0.5448577680525164\n",
      "Recall: 0.6225\n",
      "F measure: 0.5810968494749126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model precision, accuracy, recall and F1.\n",
    "def test_knn(k, test):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    for row in test:\n",
    "        # True negative case\n",
    "        if row[-1] == 0:\n",
    "            if kNN(row[:-1],k) <= 0.5:\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        # True positive case\n",
    "        else:\n",
    "            if kNN(row[:-1],k) > 0.5:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "\n",
    "    precision = true_positives/(true_positives + false_positives)\n",
    "    recall = true_positives/(true_positives + false_negatives)\n",
    "    accuracy = (true_positives + true_negatives)/(true_positives + false_negatives + true_negatives + false_positives)\n",
    "    f = 2 * precision * recall/(precision + recall)\n",
    "    print('For k =',k)\n",
    "    print('Accuracy:',accuracy)\n",
    "    print('Precision:',precision)\n",
    "    print('Recall:',recall)\n",
    "    print('F measure:',f)\n",
    "    print()\n",
    "    return \n",
    "\n",
    "for k in range(1, 10):\n",
    "    test_knn(k, test)\n",
    "for k in range(20, 100, 10):\n",
    "    test_knn(k, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "#### Now instead of having a single multivariate Gaussian distribution per class, each class is  going  to  be  generated  by  a  mixture  of  3  Gaussians.   For  each  class,  we’ll  define 3  Gaussians,  with  the  first  Gaussian  of  the  first  class  sharing  the  covariance  matrix with  the  first  Gaussian  of  the  second  class  and  so  on.   For  both  the  classes,  fix  the mixture probability as (0.1,0.42,0.48) i.e.  the sample has arisen from first Gaussian with probability 0.1, second with probability 0.42 and so on.  Mean for three Gaussians in the positive class are given as DS2-c1-m1, DS2-c1-m2, DS2-c1-m3.  Mean for three Gaussians in the negative class are gives as DS2-c2-m1, DS2-c2-m2, DS2-c2-m3.  Corresponding 3 covariance  matrices  are  given  as  DS2-cov-1,  DS2-cov-2  and  DS2-cov-3.   Now  sample from this distribution and generate the dataset similar to question 1.  Call this dataset as DS2, and submit it with your code.  Follow the instructions from Assignment 1 for data submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the mean and covariance data from file.\n",
    "ds2_c1_mean_1 = np.loadtxt(\"Datasets/DS2_c1_m1.txt\", delimiter = ',', usecols=range(20))\n",
    "ds2_c1_mean_2 = np.loadtxt(\"Datasets/DS2_c1_m2.txt\", delimiter = ',', usecols=range(20))\n",
    "ds2_c1_mean_3 = np.loadtxt(\"Datasets/DS2_c1_m3.txt\", delimiter = ',', usecols=range(20))\n",
    "\n",
    "ds2_c2_mean_1 = np.loadtxt(\"Datasets/DS2_c2_m1.txt\", delimiter = ',', usecols=range(20))\n",
    "ds2_c2_mean_2 = np.loadtxt(\"Datasets/DS2_c2_m2.txt\", delimiter = ',', usecols=range(20))\n",
    "ds2_c2_mean_3 = np.loadtxt(\"Datasets/DS2_c2_m3.txt\", delimiter = ',', usecols=range(20))\n",
    "\n",
    "ds2_cov1 = np.loadtxt(\"Datasets/DS2_Cov1.txt\", delimiter = ',', usecols= range(20))\n",
    "ds2_cov2 = np.loadtxt(\"Datasets/DS2_Cov2.txt\", delimiter = ',', usecols= range(20))\n",
    "ds2_cov3 = np.loadtxt(\"Datasets/DS2_Cov3.txt\", delimiter = ',', usecols= range(20))\n",
    "\n",
    "# Generate the multivariate gaussian distributions and sample each 2000 times.\n",
    "num = 2000 # number of total examples to generate\n",
    "pop_dist1 = int(0.1 * num)\n",
    "pop_dist2 = int(0.42 * num)\n",
    "pop_dist3 = int(0.48 * num)\n",
    "\n",
    "negative_class1 = np.random.multivariate_normal(ds2_c1_mean_1,ds2_cov1,pop_dist1)\n",
    "negative_class2 = np.random.multivariate_normal(ds2_c1_mean_2,ds2_cov2,pop_dist2)\n",
    "negative_class3 = np.random.multivariate_normal(ds2_c1_mean_3,ds2_cov3,pop_dist3)\n",
    "\n",
    "positive_class1 = np.random.multivariate_normal(ds2_c2_mean_1,ds2_cov1,pop_dist1)\n",
    "positive_class2 = np.random.multivariate_normal(ds2_c2_mean_2,ds2_cov2,pop_dist2)\n",
    "positive_class3 = np.random.multivariate_normal(ds2_c2_mean_3,ds2_cov3,pop_dist3)\n",
    "\n",
    "# Stack the three sample arrays for each class\n",
    "positive_class = np.append(positive_class1,positive_class2,axis=0)\n",
    "positive_class = np.append(positive_class,positive_class3,axis=0)\n",
    "\n",
    "negative_class = np.append(negative_class1, negative_class2,axis=0)\n",
    "negative_class = np.append(negative_class,negative_class3,axis=0)\n",
    "\n",
    "# Add labels to the data (add a 0 or a 1 on the 21-st column for labeling)\n",
    "negative_class = np.insert(negative_class, 20, 0,axis=1)\n",
    "positive_class = np.insert(positive_class, 20, 1,axis=1)\n",
    "\n",
    "# Sheffle datasets\n",
    "np.random.shuffle(negative_class)\n",
    "np.random.shuffle(positive_class)\n",
    "\n",
    "# Create train, validation and test sets from both sample distributions.\n",
    "test_2 = np.append(negative_class[:400], positive_class[:400],axis=0)\n",
    "validation_2 = np.append(negative_class[400:800], positive_class[400:800],axis=0)\n",
    "train_2 = np.append(negative_class[800:], positive_class[800:],axis=0)\n",
    "\n",
    "# Save data\n",
    "np.savetxt('DS2_train.csv', train_2, delimiter = ',')\n",
    "np.savetxt('DS2_validation.csv', validation_2, delimiter = ',')\n",
    "np.savetxt('DS2_test.csv', test_2, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "####  Now perform the experiments in questions 2 and 3 again, but now using DS2.\n",
    "1.  Estimate  the  parameters  of  the  GDA  model  using  the  maximum  likelihood  approach.\n",
    "    1.  For DS1, report the best fit accuracy, precision, recall and F-measure achieved by the classifier.\n",
    "    2.  Report the coefficients learnt.\n",
    "2.  Does k-NN  classifier  perform  better  than  GDA  or  worse?   Are  there  particular values of k which perform better?  Why does this happen ?\n",
    "3.  Report the best fit accuracy, precision, recall and f-measure achieved by this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDA Model Test\n",
      "\n",
      "Coefficients learned:\n",
      "w0: 0.09192696230167563\n",
      "w: [ 0.08120049  0.03562738 -0.0804858   0.02237502  0.10118552 -0.01249639\n",
      "  0.08101135 -0.0393756  -0.05386362 -0.03594021 -0.13171094  0.06628183\n",
      " -0.02801044 -0.0168307  -0.02945508 -0.11210141 -0.00057707  0.0367228\n",
      "  0.08563488 -0.05488231]\n",
      "\n",
      "Accuracy: 0.57125\n",
      "Precision: 0.571072319201995\n",
      "Recall: 0.5725\n",
      "F measure: 0.571785268414482\n",
      "\n",
      "\n",
      "k-NN Model Test\n",
      "\n",
      "For k = 1\n",
      "Accuracy: 0.5025\n",
      "Precision: 0.5024154589371981\n",
      "Recall: 0.52\n",
      "F measure: 0.5110565110565112\n",
      "\n",
      "For k = 2\n",
      "Accuracy: 0.51\n",
      "Precision: 0.5185185185185185\n",
      "Recall: 0.28\n",
      "F measure: 0.36363636363636365\n",
      "\n",
      "For k = 3\n",
      "Accuracy: 0.51375\n",
      "Precision: 0.5141388174807198\n",
      "Recall: 0.5\n",
      "F measure: 0.5069708491761724\n",
      "\n",
      "For k = 4\n",
      "Accuracy: 0.525\n",
      "Precision: 0.5352112676056338\n",
      "Recall: 0.38\n",
      "F measure: 0.4444444444444444\n",
      "\n",
      "For k = 5\n",
      "Accuracy: 0.52375\n",
      "Precision: 0.5223529411764706\n",
      "Recall: 0.555\n",
      "F measure: 0.5381818181818182\n",
      "\n",
      "For k = 6\n",
      "Accuracy: 0.52125\n",
      "Precision: 0.5271565495207667\n",
      "Recall: 0.4125\n",
      "F measure: 0.4628330995792426\n",
      "\n",
      "For k = 7\n",
      "Accuracy: 0.51875\n",
      "Precision: 0.5182481751824818\n",
      "Recall: 0.5325\n",
      "F measure: 0.5252774352651047\n",
      "\n",
      "For k = 8\n",
      "Accuracy: 0.5275\n",
      "Precision: 0.5337423312883436\n",
      "Recall: 0.435\n",
      "F measure: 0.47933884297520657\n",
      "\n",
      "For k = 9\n",
      "Accuracy: 0.5125\n",
      "Precision: 0.5119047619047619\n",
      "Recall: 0.5375\n",
      "F measure: 0.5243902439024389\n",
      "\n",
      "For k = 20\n",
      "Accuracy: 0.50625\n",
      "Precision: 0.5068493150684932\n",
      "Recall: 0.4625\n",
      "F measure: 0.48366013071895425\n",
      "\n",
      "For k = 30\n",
      "Accuracy: 0.505\n",
      "Precision: 0.5052631578947369\n",
      "Recall: 0.48\n",
      "F measure: 0.4923076923076923\n",
      "\n",
      "For k = 40\n",
      "Accuracy: 0.49875\n",
      "Precision: 0.49868766404199477\n",
      "Recall: 0.475\n",
      "F measure: 0.48655569782330343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load datasets from file\n",
    "test_2 = np.loadtxt('DS2_test.csv',delimiter = ',')\n",
    "validation_2 = np.loadtxt('DS2_validation.csv',delimiter = ',')\n",
    "train_2 = np.loadtxt('DS2_train.csv',delimiter = ',')\n",
    "\n",
    "# Test GDA\n",
    "print('GDA Model Test\\n')\n",
    "w0_2, w_2 = GDA(test_2)\n",
    "print()\n",
    "test_GDA(test_2, w0_2, w_2)\n",
    "print('\\n')\n",
    "\n",
    "# Test k-NN\n",
    "print('k-NN Model Test\\n')\n",
    "for k in range(1, 10):\n",
    "    test_knn(k, test_2)\n",
    "for k in range(20, 50, 10):\n",
    "    test_knn(k, test_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
