{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 551: Applied Machine Learning\n",
    "## Assignment 3: Sentiment Classification - Yelp & IMDB\n",
    "### Author: Antonios Valkanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string, random, os\n",
    "import sklearn.naive_bayes\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Most of the algorithms described in the class expects input as a vector. However, the reviews are natural language text of varying number of words. So the first step would be to convert this varying length movie review to a fixed length vector representation. We will consider two different ways of vectorizing the natural language text: binary bag-of-words representation and frequency bag-of-words representation (as explained in the end of the assignment). Convert both the datasets into both these representations and turn in the converted datasets. Instruction for dataset format is given in the end of the assignment (do not include the dataset in the printed report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data with pandas reading in from file with utf-8.\n",
    "def pre_process (file_name):\n",
    "    temp = []\n",
    "    with open(file_name, encoding=\"UTF-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            full_line = line.split(\"\\t\")\n",
    "            comment_score = (full_line[1])\n",
    "            comment_text = str.lower(full_line[0]).translate(str.maketrans('', '', string.punctuation))\n",
    "            temp.append([comment_text,comment_score])\n",
    "    return np.asarray(temp)\n",
    "\n",
    "yelp_train = pd.DataFrame(pre_process(\"Datasets/yelp-train.txt\")).rename(columns={0: \"review\", 1: \"rating\"})\n",
    "yelp_valid = pd.DataFrame(pre_process(\"Datasets/yelp-valid.txt\")).rename(columns={0: \"review\", 1: \"rating\"})\n",
    "yelp_test = pd.DataFrame(pre_process(\"Datasets/yelp-test.txt\")).rename(columns={0: \"review\", 1: \"rating\"})\n",
    "\n",
    "imdb_train = pd.DataFrame(pre_process(\"Datasets/imdb-train.txt\")).rename(columns={0: \"review\", 1: \"rating\"})\n",
    "imdb_valid = pd.DataFrame(pre_process(\"Datasets/imdb-valid.txt\")).rename(columns={0: \"review\", 1: \"rating\"})\n",
    "imdb_test = pd.DataFrame(pre_process(\"Datasets/imdb-test.txt\")).rename(columns={0: \"review\", 1: \"rating\"})\n",
    "\n",
    "\n",
    "count_vectorizer_yelp = CountVectorizer(max_features = 10000, binary = True) \n",
    "count_vectorizer_imdb = CountVectorizer(max_features = 10000, binary = True) \n",
    "\n",
    "\n",
    "# Vectorize datasets\n",
    "vectorized_yelp_train = count_vectorizer_yelp.fit_transform(yelp_train['review'])\n",
    "vectorized_yelp_test = count_vectorizer_yelp.transform(yelp_test['review'])\n",
    "vectorized_yelp_valid = count_vectorizer_yelp.transform(yelp_valid['review'])\n",
    "vectorized_imdb_train = count_vectorizer_imdb.fit_transform(imdb_train['review'])\n",
    "vectorized_imdb_test = count_vectorizer_imdb.transform(imdb_test['review'])\n",
    "vectorized_imdb_valid = count_vectorizer_imdb.transform(imdb_valid['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_vocabulary = count_vectorizer_yelp.get_feature_names()\n",
    "# From https://stackoverflow.com/questions/27488446/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-countvectorizer/27488756\n",
    "# Retrieved how to use cv_fit.sum(axis = 0) as a quick way to count the word frequency.\n",
    "yelp_frequency = np.asarray(vectorized_yelp_train.sum(axis=0))\n",
    "yelp_frequency = np.ndarray.flatten(yelp_frequency)\n",
    "\n",
    "imdb_vocabulary = count_vectorizer_imdb.get_feature_names()\n",
    "imdb_frequency = np.asarray(vectorized_imdb_train.sum(axis=0))\n",
    "imdb_frequency = np.ndarray.flatten(imdb_frequency)\n",
    "\n",
    "with open(\"yelp-vocab.txt\",'w+') as f:\n",
    "    for i in range(10000):\n",
    "        f.write(\"{}\\t{}\\t{}\\n\".format(yelp_vocabulary[i],i,yelp_frequency[i]))\n",
    "        \n",
    "with open(\"imdb-vocab.txt\",'w+') as f:\n",
    "    for i in range(10000):\n",
    "        f.write(\"{}\\t{}\\t{}\\n\".format(imdb_vocabulary[i],i,imdb_frequency[i]))\n",
    "\n",
    "yelp_vocab_dict = count_vectorizer_yelp.vocabulary_  \n",
    "imdb_vocab_dict = count_vectorizer_imdb.vocabulary_ \n",
    "\n",
    "def write_file(filename, df, dictionary):\n",
    "    with open(filename,'w+') as f:\n",
    "        i = -1\n",
    "        for text, rating in zip(df['review'], df['rating']):\n",
    "            review = \"\"\n",
    "            i += 1\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                this_word = dictionary.get(word,None)           \n",
    "                if this_word is not None:\n",
    "                    review += str(this_word)+ \" \"                \n",
    "            f.write(\"{}\\t{}\\n\".format(review, rating))\n",
    "            \n",
    "write_file(\"yelp-train.txt\", yelp_train, yelp_vocab_dict)\n",
    "write_file(\"yelp-valid.txt\", yelp_valid, yelp_vocab_dict)\n",
    "write_file(\"yelp-test.txt\", yelp_test, yelp_vocab_dict)\n",
    "\n",
    "write_file(\"imdb-train.txt\", imdb_train, imdb_vocab_dict)\n",
    "write_file(\"imdb-valid.txt\", imdb_valid, imdb_vocab_dict)\n",
    "write_file(\"imdb-test.txt\", imdb_test, imdb_vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "For this question, we will focus on the yelp dataset with binary bag-of-words (BBoW) representation. We will use the F1-measure as the evaluation metric for the entire assignment.\n",
    "* As a baseline, report the performance of the random classifier (a classifier which classifies a review into an uniformly random class) and the majority-class classifier (a classifier which computes the majority class in the training set and classifies all test instances as that majority class).\n",
    "* Now train Naive Bayes, Decision Trees, and Linear SVM for this task. Note: You should do a thorough hyper-parameter tuning by using the given validation set. Also, note that you should use the appropriate naive Bayes classifier for binary input features (also called Bernoulli naive Bayes).\n",
    "* Report the list of hyper-parameters you considered for each classifier, the range of the individual hyper-parameters and the best value for these hyper-parameters chosen based on the validation set performance1.\n",
    "* Report training, validation, and test F1-measure for all the classifiers (with best hyper-parameter configuration).\n",
    "* Comment about the performance of different classifiers. Why did a particular classifier performe better than the others? What was the role of that hyperparameter that fetched you the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "\n",
    "yelp_train['rating'] = yelp_train['rating'].astype(np.int32)\n",
    "yelp_valid['rating'] = yelp_valid['rating'].astype(int)\n",
    "yelp_test['rating'] = yelp_test['rating'].astype(int)\n",
    "\n",
    "imdb_train['rating'] = imdb_train['rating'].astype(int)\n",
    "imdb_valid['rating'] = imdb_valid['rating'].astype(int)\n",
    "imdb_test['rating'] = imdb_test['rating'].astype(int)\n",
    "\n",
    "# Dummy\n",
    "random_dummy = DummyClassifier(strategy = 'uniform')\n",
    "majority_dummy = DummyClassifier(strategy = 'most_frequent')\n",
    "\n",
    "# Naive-Bayes, SVM, Tree\n",
    "# Using Grid search as proposed in: https://stackoverflow.com/questions/33830959/multinomial-naive-bayes-parameter-alpha-setting-scikit-learn\n",
    "nb_params = ParameterGrid({'alpha':[1e-5, 1e-4, 1e-3,0.01,0.1,0.3,0.6,1]})\n",
    "tree_params = ParameterGrid({'criterion':['gini','entropy'],'max_depth':[None,5,10,100,500],'min_samples_split':[3,5,10,15,20]})\n",
    "svm_params = ParameterGrid({'loss':['hinge','squared_hinge'],'C':[0.5,1.0,1.5,2.0,5.0,10.0,100.0]})\n",
    "\n",
    "classifiers = [(BernoulliNB, nb_params), (DecisionTreeClassifier, tree_params), (svm.LinearSVC, svm_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(model, yelp_dataset = True):\n",
    "    if yelp_dataset:\n",
    "        model.fit(vectorized_yelp_train, yelp_train['rating'].astype(int))\n",
    "        valid = model.predict(vectorized_yelp_valid)\n",
    "        f1_valid = f1_score(yelp_valid['rating'].astype(int), valid, average='micro')\n",
    "    else:\n",
    "        model.fit(vectorized_imdb_train, imdb_train['rating'].astype(int))\n",
    "        valid = model.predict(vectorized_imdb_valid)\n",
    "        f1_valid = f1_score(imdb_valid['rating'].astype(int), valid, average='binary')\n",
    "    return f1_valid\n",
    "\n",
    "def model_train_valid_test(model, yelp_dataset = True):\n",
    "    if yelp_dataset:\n",
    "        model.fit(vectorized_yelp_train, yelp_train['rating'].astype(int))\n",
    "        train = model.predict(vectorized_yelp_train)\n",
    "        valid = model.predict(vectorized_yelp_valid)\n",
    "        test = model.predict(vectorized_yelp_test)\n",
    "        f1_train = f1_score(yelp_train['rating'].astype(int), train, average='micro')\n",
    "        f1_valid = f1_score(yelp_valid['rating'].astype(int), valid, average='micro')\n",
    "        f1_test = f1_score(yelp_test['rating'].astype(int), test, average='micro')\n",
    "    else:\n",
    "        model.fit(vectorized_imdb_train, imdb_train['rating'].astype(int))\n",
    "        train = model.predict(vectorized_imdb_train)\n",
    "        valid = model.predict(vectorized_imdb_valid)\n",
    "        test = model.predict(vectorized_imdb_test)\n",
    "        f1_train = f1_score(imdb_train['rating'].astype(int), train, average='binary')\n",
    "        f1_valid = f1_score(imdb_valid['rating'].astype(int), valid, average='binary')\n",
    "        f1_test = f1_score(imdb_test['rating'].astype(int), test, average='binary')\n",
    "    return f1_train, f1_valid, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Tests\n",
      "\n",
      "Random Dummy Classifier\n",
      "0.185\n",
      "Majority Dummy Classifier\n",
      "0.356\n",
      "Naive Bayes Classifier\n",
      "Decision Tree Classifier\n",
      "SVM Classifier\n",
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "Test for: {'alpha': 1e-05}\n",
      "F1 Score Validation:0.404\n",
      "\n",
      "Test for: {'alpha': 0.0001}\n",
      "F1 Score Validation:0.412\n",
      "\n",
      "Test for: {'alpha': 0.001}\n",
      "F1 Score Validation:0.426\n",
      "\n",
      "Test for: {'alpha': 0.01}\n",
      "F1 Score Validation:0.428\n",
      "\n",
      "Test for: {'alpha': 0.1}\n",
      "F1 Score Validation:0.41\n",
      "\n",
      "Test for: {'alpha': 0.3}\n",
      "F1 Score Validation:0.398\n",
      "\n",
      "Test for: {'alpha': 0.6}\n",
      "F1 Score Validation:0.394\n",
      "\n",
      "Test for: {'alpha': 1}\n",
      "F1 Score Validation:0.388\n",
      "\n",
      "Best params for Validation: {'alpha': 0.01}\n",
      "Best F1 Score on Validation: 0.428\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.31\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.309\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.321\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.329\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.332\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.377\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.377\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.376\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.376\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.376\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.401\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.399\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.401\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.401\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.402\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.314\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.31\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.331\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.333\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.323\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.323\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.32\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.316\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.331\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.334\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.352\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.348\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.356\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.351\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.351\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.388\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.389\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.389\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.39\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.39\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.409\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.41\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.413\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.40700000000000003\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.412\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.341\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.348\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.354\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.336\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.34\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.367\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.348\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.354\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.344\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.349\n",
      "\n",
      "Best params for Validation: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
      "Best F1 Score on Validation: 0.413\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "Test for: {'C': 0.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.457\n",
      "\n",
      "Test for: {'C': 0.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.465\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.457\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.456\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.44800000000000006\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.446\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.44\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.436\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.433\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.443\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.43799999999999994\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.447\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.434\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.446\n",
      "\n",
      "Best params for Validation: {'C': 0.5, 'loss': 'squared_hinge'}\n",
      "Best F1 Score on Validation: 0.465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find best params for a classifier \n",
    "def optimize_parameters(classifier, grid, yelp = True):\n",
    "    best_f1 = 0.0\n",
    "    optimal_params = None\n",
    "    for params in grid:\n",
    "        print(\"Test for:\", params)\n",
    "        f1 = model_validation(classifier(**params), yelp)\n",
    "        print(\"F1 :{}\\n\".format(f1))\n",
    "        if f1>best_f1:\n",
    "            best_f1=f1\n",
    "            optimal_params=params       \n",
    "    print(\"Best params: {}\".format(optimal_params))\n",
    "    print(\"Best F1 Score: {}\\n\".format(best_f1))\n",
    "    return classifier(**optimal_params)\n",
    "\n",
    "print('Classifier Tests\\n')\n",
    "print('Random Dummy Classifier')\n",
    "print(model_validation(random_dummy))\n",
    "print('Majority Dummy Classifier')\n",
    "print(model_validation(majority_dummy))\n",
    "print('Naive Bayes Classifier')\n",
    "print('Decision Tree Classifier')\n",
    "print('SVM Classifier')\n",
    "\n",
    "for param in classifiers: # cycle through the classifiers and parameters\n",
    "    clf = param[0]\n",
    "    param_grid = param[1]\n",
    "    print(clf)    \n",
    "    best_clf = optimize_parameters(clf,param_grid, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal NB f1 for train, valid, test (0.7478571428571429, 0.428, 0.4395)\n",
      "Optimal Tree f1 for train, valid, test (0.5022857142857143, 0.413, 0.3895)\n",
      "Optimal SVM f1 for train, valid, test (0.9931428571428571, 0.465, 0.4475)\n"
     ]
    }
   ],
   "source": [
    "# Optimal Parameters derived from above:\n",
    "# NB: {'aplha': 0.01}\n",
    "# Tree: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
    "# Linear SVC: {'C': 0.5, 'loss': 'squared_hinge'}\n",
    "\n",
    "optimized_bayes = BernoulliNB(alpha = 0.01)\n",
    "optimized_tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 10, min_samples_split = 10, random_state = 42)\n",
    "optimized_svm = svm.LinearSVC(C = 0.5, loss = 'squared_hinge', random_state = 42)\n",
    "\n",
    "print('Optimal NB f1 for train, valid, test', model_train_valid_test(optimized_bayes))\n",
    "print('Optimal Tree f1 for train, valid, test', model_train_valid_test(optimized_tree))\n",
    "print('Optimal SVM f1 for train, valid, test', model_train_valid_test(optimized_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Now we will repeat question 2 but with frequency bag-of-words (FBoW) representation.\n",
    "* Train Naive Bayes, Decision Trees, and Linear SVM for this task.\n",
    "* Report the list of hyper-parameters you considered for each classifier, the range of the individual hyper-parameters and the best value for these hyper-parameters chosen based on the validation set performance.\n",
    "* Report training, validation, and test F1-measure for all the classifiers (with best hyper-parameter configuration).\n",
    "* Comment about the performance of different classifiers. Why did a particular classifier perform better than the others? What was the role of that hyperparameter that fetched you the best results.\n",
    "* Compare the performance with the binary bag-of-words based classifiers. Why the difference in performance? Give a brief explanation comparing BBoW Naive Bayes and FBoW Naive Bayes and similarly for Decision Trees and Linear SVM.\n",
    "* Which representation is better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_count_vectorizer_yelp = CountVectorizer(max_features = 10000, binary = False) \n",
    "freq_count_vectorizer_imdb = CountVectorizer(max_features = 10000, binary = False) \n",
    "\n",
    "vectorized_yelp_train = freq_count_vectorizer_yelp.fit_transform(yelp_train['review'])\n",
    "vectorized_yelp_test = freq_count_vectorizer_yelp.transform(yelp_test['review'])\n",
    "vectorized_yelp_valid = freq_count_vectorizer_yelp.transform(yelp_valid['review'])\n",
    "\n",
    "normalize = Normalizer(norm='l1')\n",
    "\n",
    "vectorized_yelp_train = normalize.transform(vectorized_yelp_train)\n",
    "vectorized_yelp_test = normalize.transform(vectorized_yelp_test)\n",
    "vectorized_yelp_valid = normalize.transform(vectorized_yelp_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Tests\n",
      "\n",
      "Random Dummy Classifier\n",
      "0.212\n",
      "Majority Dummy Classifier\n",
      "0.356\n",
      "Naive Bayes Classifier\n",
      "Decision Tree Classifier\n",
      "SVM Classifier\n",
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "Test for: {'alpha': 1e-05}\n",
      "F1 Score Validation:0.3970000000000001\n",
      "\n",
      "Test for: {'alpha': 0.0001}\n",
      "F1 Score Validation:0.404\n",
      "\n",
      "Test for: {'alpha': 0.001}\n",
      "F1 Score Validation:0.421\n",
      "\n",
      "Test for: {'alpha': 0.01}\n",
      "F1 Score Validation:0.425\n",
      "\n",
      "Test for: {'alpha': 0.1}\n",
      "F1 Score Validation:0.402\n",
      "\n",
      "Test for: {'alpha': 0.3}\n",
      "F1 Score Validation:0.393\n",
      "\n",
      "Test for: {'alpha': 0.6}\n",
      "F1 Score Validation:0.391\n",
      "\n",
      "Test for: {'alpha': 1}\n",
      "F1 Score Validation:0.383\n",
      "\n",
      "Best params for Validation: {'alpha': 0.01}\n",
      "Best F1 Score on Validation: 0.425\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.329\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.342\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.323\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.333\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.336\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.394\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.394\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.393\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.396\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.396\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.387\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.389\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.389\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.39\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.38499999999999995\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.33\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.339\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.329\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.333\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.334\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.339\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.331\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.339\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.333\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.33\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.337\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.349\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.354\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.347\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.346\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.399\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.4000000000000001\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.4000000000000001\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.399\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.4000000000000001\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.386\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.391\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.389\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.393\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.39\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.332\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.335\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.351\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.35\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.347\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.342\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.347\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.345\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.352\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.344\n",
      "\n",
      "Best params for Validation: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
      "Best F1 Score on Validation: 0.4000000000000001\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "Test for: {'C': 0.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.446\n",
      "\n",
      "Test for: {'C': 0.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.43099999999999994\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.447\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.44800000000000006\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.44800000000000006\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.457\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.451\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.465\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.464\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.488\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.47\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.483\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.488\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.491\n",
      "\n",
      "Best params for Validation: {'C': 100.0, 'loss': 'squared_hinge'}\n",
      "Best F1 Score on Validation: 0.491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classifier Tests\\n')\n",
    "print('Random Dummy Classifier')\n",
    "print(model_validation(random_dummy))\n",
    "print('Majority Dummy Classifier')\n",
    "print(model_validation(majority_dummy))\n",
    "print('Naive Bayes Classifier')\n",
    "print('Decision Tree Classifier')\n",
    "print('SVM Classifier')\n",
    "\n",
    "for param in classifiers: # cycle through the classifiers and parameters\n",
    "    clf = param[0]\n",
    "    param_grid = param[1]\n",
    "    print(clf)    \n",
    "    best_clf = optimize_parameters(clf,param_grid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal NB f1 for train, valid, test (0.7482857142857143, 0.425, 0.437)\n",
      "Optimal Tree f1 for train, valid, test (0.5434285714285715, 0.39, 0.3785)\n",
      "Optimal SVM f1 for train, valid, test (0.48328571428571426, 0.43099999999999994, 0.4545)\n"
     ]
    }
   ],
   "source": [
    "# Optimal Parameters derived from above:\n",
    "# NB: {'aplha': 0.01}\n",
    "# Tree: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
    "# Linear SVC: {'C': 0.5, 'loss': 'squared_hinge'}\n",
    "\n",
    "optimized_bayes = BernoulliNB(alpha = 0.01)\n",
    "optimized_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 10, random_state = 42)\n",
    "optimized_svm = svm.LinearSVC(C = 0.5, loss = 'squared_hinge', random_state = 42)\n",
    "\n",
    "print('Optimal NB f1 for train, valid, test', model_train_valid_test(optimized_bayes))\n",
    "print('Optimal Tree f1 for train, valid, test', model_train_valid_test(optimized_tree))\n",
    "print('Optimal SVM f1 for train, valid, test', model_train_valid_test(optimized_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Repeat Question 2 for IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Tests\n",
      "\n",
      "Random Dummy Classifier\n",
      "0.223 False\n",
      "Majority Dummy Classifier\n",
      "0.356 False\n",
      "Naive Bayes Classifier\n",
      "Decision Tree Classifier\n",
      "SVM Classifier\n",
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "Test for: {'alpha': 1e-05}\n",
      "F1 Score Validation:0.842041189002739\n",
      "\n",
      "Test for: {'alpha': 0.0001}\n",
      "F1 Score Validation:0.8422440904940651\n",
      "\n",
      "Test for: {'alpha': 0.001}\n",
      "F1 Score Validation:0.8421266233766233\n",
      "\n",
      "Test for: {'alpha': 0.01}\n",
      "F1 Score Validation:0.8423615337796714\n",
      "\n",
      "Test for: {'alpha': 0.1}\n",
      "F1 Score Validation:0.8422440904940651\n",
      "\n",
      "Test for: {'alpha': 0.3}\n",
      "F1 Score Validation:0.8418916176172113\n",
      "\n",
      "Test for: {'alpha': 0.6}\n",
      "F1 Score Validation:0.8411328799106691\n",
      "\n",
      "Test for: {'alpha': 1}\n",
      "F1 Score Validation:0.840682788051209\n",
      "\n",
      "Best params for Validation: {'alpha': 0.01}\n",
      "Best F1 Score on Validation: 0.8423615337796714\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6990947975728637\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6924151696606786\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6926\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6878466969238527\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6883208020050126\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7307919871523963\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7308245243128965\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7306846999154692\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7307009385304811\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.7308537616229924\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7298285071689627\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7286196549137285\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7298311444652907\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7292527224934285\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.7306286892157782\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6995622761639474\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6930732484076434\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6904191616766466\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.694930506949305\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6958437656484726\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6934183013043911\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6947410438080032\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6935548438751\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6856217747647475\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6891498544615076\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6930417495029821\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6954264030357499\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6915457728864433\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6946626384692849\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.696412420862225\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7291542963779795\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7296770365347122\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7295075019072645\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7294456687574165\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.7295853472398881\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7339348186337692\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7344977358839294\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7362982100018454\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7336163928373639\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.735995562950638\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6962948167382403\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6907888142728276\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6947726817544563\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7015237526142815\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6901266840941082\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6896345116836429\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6936208445642408\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6846426761277243\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6986383660392471\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6976\n",
      "\n",
      "Best params for Validation: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
      "Best F1 Score on Validation: 0.7362982100018454\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "Test for: {'C': 0.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8442885771543087\n",
      "\n",
      "Test for: {'C': 0.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8468016843793864\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8403647660086181\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8432334969448062\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8388905577250425\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8416307723129319\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8377078741735122\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8409455128205129\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8381543389050146\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8387937080452861\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8381543389050146\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8389228150966062\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8381543389050146\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8382382382382382\n",
      "\n",
      "Best params for Validation: {'C': 0.5, 'loss': 'squared_hinge'}\n",
      "Best F1 Score on Validation: 0.8468016843793864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classifier Tests\\n')\n",
    "print('Random Dummy Classifier')\n",
    "print(model_validation(random_dummy), False)\n",
    "print('Majority Dummy Classifier')\n",
    "print(model_validation(majority_dummy), False)\n",
    "print('Naive Bayes Classifier')\n",
    "print('Decision Tree Classifier')\n",
    "print('SVM Classifier')\n",
    "\n",
    "for param in classifiers: # cycle through the classifiers and parameters\n",
    "    clf = param[0]\n",
    "    param_grid = param[1]\n",
    "    print(clf)    \n",
    "    best_clf = optimize_parameters(clf,param_grid, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal NB f1 for train, valid, test (0.8718817787418656, 0.8423615337796714, 0.8318656900666611)\n",
      "Optimal Tree f1 for train, valid, test (0.7708098635623983, 0.7296233839235526, 0.7262984336356142)\n",
      "Optimal SVM f1 for train, valid, test (1.0, 0.8468016843793864, 0.836321341194401)\n"
     ]
    }
   ],
   "source": [
    "# Optimal Parameters derived from above:\n",
    "# NB: {'aplha': 0.01}\n",
    "# Tree: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
    "# Linear SVC: {'C': 0.5, 'loss': 'squared_hinge'}\n",
    "\n",
    "optimized_bayes = BernoulliNB(alpha = 0.01)\n",
    "optimized_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 10, random_state = 42)\n",
    "optimized_svm = svm.LinearSVC(C = 0.5, loss = 'squared_hinge', random_state = 42)\n",
    "\n",
    "print('Optimal NB f1 for train, valid, test', model_train_valid_test(optimized_bayes, False))\n",
    "print('Optimal Tree f1 for train, valid, test', model_train_valid_test(optimized_tree, False))\n",
    "print('Optimal SVM f1 for train, valid, test', model_train_valid_test(optimized_svm, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Repeat Question 3 for IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_count_vectorizer_imdb = CountVectorizer(max_features = 10000, binary = False) \n",
    "\n",
    "vectorized_imdb_train = freq_count_vectorizer_imdb.fit_transform(imdb_train['review'])\n",
    "vectorized_imdb_test = freq_count_vectorizer_imdb.transform(imdb_test['review'])\n",
    "vectorized_imdb_valid = freq_count_vectorizer_imdb.transform(imdb_valid['review'])\n",
    "\n",
    "normalize = Normalizer(norm='l1')\n",
    "\n",
    "vectorized_imdb_train = normalize.transform(vectorized_imdb_train)\n",
    "vectorized_imdb_test = normalize.transform(vectorized_imdb_test)\n",
    "vectorized_imdb_valid = normalize.transform(vectorized_imdb_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Tests\n",
      "\n",
      "Random Dummy Classifier\n",
      "0.213 False\n",
      "Majority Dummy Classifier\n",
      "0.356 False\n",
      "Naive Bayes Classifier\n",
      "Decision Tree Classifier\n",
      "SVM Classifier\n",
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "Test for: {'alpha': 1e-05}\n",
      "F1 Score Validation:0.839167935058346\n",
      "\n",
      "Test for: {'alpha': 0.0001}\n",
      "F1 Score Validation:0.8393382726073279\n",
      "\n",
      "Test for: {'alpha': 0.001}\n",
      "F1 Score Validation:0.8397442403328935\n",
      "\n",
      "Test for: {'alpha': 0.01}\n",
      "F1 Score Validation:0.8398497004163704\n",
      "\n",
      "Test for: {'alpha': 0.1}\n",
      "F1 Score Validation:0.8402234636871508\n",
      "\n",
      "Test for: {'alpha': 0.3}\n",
      "F1 Score Validation:0.8403088175538399\n",
      "\n",
      "Test for: {'alpha': 0.6}\n",
      "F1 Score Validation:0.8404352689921692\n",
      "\n",
      "Test for: {'alpha': 1}\n",
      "F1 Score Validation:0.8398982188295165\n",
      "\n",
      "Best params for Validation: {'alpha': 0.6}\n",
      "Best F1 Score on Validation: 0.8404352689921692\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6914176476506692\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6864440868865648\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6876445159344527\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6902261712439418\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6884255061952251\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7361226871145191\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7358333333333332\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7360173376677502\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7360173376677502\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.7360786928976326\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7430513200313672\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7437930133286872\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7422590492804186\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7428770584647556\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.7433103808942735\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6916825524229959\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6895372233400402\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6878519710378117\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6880032206119162\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6834728244428758\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6861933138066862\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6911278195488721\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6882566585956416\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6862645717181957\n",
      "\n",
      "Test for: {'criterion': 'gini', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6850616535273903\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6927140436559355\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6927374301675978\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6880484114977308\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6930158094856914\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6882435974994959\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7337972667057936\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7339203354297694\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7340880503144654\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7339818852733981\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.7339203354297694\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.7262516657148296\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.7268399504903361\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.7267707539984768\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.7277237168479298\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.7275500476644423\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6911559193451787\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6932318782539048\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6850917893887433\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6934965877157767\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 100, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.688967971530249\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 3}\n",
      "F1 Score Validation:0.6908982274447321\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 5}\n",
      "F1 Score Validation:0.6933679584498602\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 10}\n",
      "F1 Score Validation:0.6918592964824122\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 15}\n",
      "F1 Score Validation:0.6891918989372369\n",
      "\n",
      "Test for: {'criterion': 'entropy', 'max_depth': 500, 'min_samples_split': 20}\n",
      "F1 Score Validation:0.6845910988899073\n",
      "\n",
      "Best params for Validation: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5}\n",
      "Best F1 Score on Validation: 0.7437930133286872\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "Test for: {'C': 0.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.6941780490167587\n",
      "\n",
      "Test for: {'C': 0.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.7627334050249291\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.7200619075256337\n",
      "\n",
      "Test for: {'C': 1.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.7977374683050518\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.7394325419803127\n",
      "\n",
      "Test for: {'C': 1.5, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8128603537574515\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.7513033404132072\n",
      "\n",
      "Test for: {'C': 2.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8226185809486476\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.7992281717317897\n",
      "\n",
      "Test for: {'C': 5.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8456323081458189\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.8296339563862928\n",
      "\n",
      "Test for: {'C': 10.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8595350669818755\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'hinge'}\n",
      "F1 Score Validation:0.873136538651397\n",
      "\n",
      "Test for: {'C': 100.0, 'loss': 'squared_hinge'}\n",
      "F1 Score Validation:0.8782083044296898\n",
      "\n",
      "Best params for Validation: {'C': 100.0, 'loss': 'squared_hinge'}\n",
      "Best F1 Score on Validation: 0.8782083044296898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classifier Tests\\n')\n",
    "print('Random Dummy Classifier')\n",
    "print(model_validation(random_dummy), False)\n",
    "print('Majority Dummy Classifier')\n",
    "print(model_validation(majority_dummy), False)\n",
    "print('Naive Bayes Classifier')\n",
    "print('Decision Tree Classifier')\n",
    "print('SVM Classifier')\n",
    "\n",
    "for param in classifiers: # cycle through the classifiers and parameters\n",
    "    clf = param[0]\n",
    "    param_grid = param[1]\n",
    "    print(clf)    \n",
    "    best_clf = optimize_parameters(clf,param_grid, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal NB f1 for train, valid, test (0.86973648465091, 0.8404352689921692, 0.8298329750289399)\n",
      "Optimal Tree f1 for train, valid, test (0.7947765525246664, 0.7427874139283535, 0.7464284472870103)\n",
      "Optimal SVM f1 for train, valid, test (0.9490737666821593, 0.8784158415841584, 0.874476820664089)\n"
     ]
    }
   ],
   "source": [
    "# Optimal Parameters derived from above:\n",
    "# NB: {'aplha': 0.6}\n",
    "# Tree: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10}\n",
    "# Linear SVC: {'C': 100.0, 'loss': 'squared_hinge', 'random_state': 42}\n",
    "\n",
    "optimized_bayes = BernoulliNB(alpha = 0.6)\n",
    "optimized_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 10, min_samples_split = 10, random_state = 42)\n",
    "optimized_svm = svm.LinearSVC(C = 100.0, loss = 'squared_hinge', random_state = 42)\n",
    "\n",
    "print('Optimal NB f1 for train, valid, test', model_train_valid_test(optimized_bayes, False))\n",
    "print('Optimal Tree f1 for train, valid, test', model_train_valid_test(optimized_tree, False))\n",
    "print('Optimal SVM f1 for train, valid, test', model_train_valid_test(optimized_svm, False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
